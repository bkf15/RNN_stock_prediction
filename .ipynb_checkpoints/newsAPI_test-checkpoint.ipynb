{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "newsapi = NewsApiClient(api_key='1cd3540192e6449e935a953fb7b02fcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keyword/phrase q param should be of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f7aaee2bf0e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#get all the news articles with ibm in title/body from today (from_param is the oldest date you want articles from)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewsapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ibm'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'asdf'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2020-03-23'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#only fetch from specific sources with domains='bbc.co.uk, ...'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#response is a dict with following properties:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# r[\"status\"] - whether you get the response or not (loop until 'ok')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\newsapi\\newsapi_client.py\u001b[0m in \u001b[0;36mget_everything\u001b[1;34m(self, q, qintitle, sources, domains, exclude_domains, from_param, to, language, sort_by, page, page_size)\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[0mpayload\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"q\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"keyword/phrase q param should be of type str\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Keyword/Phrase in Title\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: keyword/phrase q param should be of type str"
     ]
    }
   ],
   "source": [
    "#get all the news articles with ibm in title/body from today (from_param is the oldest date you want articles from)\n",
    "r = newsapi.get_everything(q='ibm,asdf'}, from_param='2020-03-23', language='en')\n",
    "#only fetch from specific sources with domains='bbc.co.uk, ...'\n",
    "#response is a dict with following properties:\n",
    "# r[\"status\"] - whether you get the response or not (loop until 'ok')\n",
    "# r[\"totalResults\"]\n",
    "# r[\"articles\"][i] - dict of content for the i'th article\n",
    "#   r[\"articles\"][i][\"source\"] - where the article came from, what website\n",
    "#   r[\"articles\"][i][\"author\"]\n",
    "#   r[\"articles\"][i][\"title\"]\n",
    "#   r[\"articles\"][i][\"description\"]\n",
    "#   r[\"articles\"][i][\"url\"]\n",
    "#   r[\"articles\"][i][\"urlToImage\"]\n",
    "#   r[\"articles\"][i][\"publishedAt\"]\n",
    "#   r[\"articles\"][i][\"content\"]      #not always available, and limited to 260 characters\n",
    "\n",
    "print(r[\"articles\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "#use the url to get the news articles html, to try to scrape more of the content \n",
    "url = r[\"articles\"][1][\"url\"]\n",
    "\n",
    "r1 = requests.get(url)\n",
    "coverpage = r1.content\n",
    "\n",
    "soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "#going to need to find how to scrape each individual site. will take some time, as each site will be different. come up\n",
    "# with a list of sites and how to scrape them, limit the search to these sites \n",
    "\n",
    "coverpage_news = soup1.find_all('script')\n",
    "\n",
    "print(type(coverpage_news[0]))\n",
    "#print(soup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
